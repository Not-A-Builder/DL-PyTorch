{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of CNNs?\n",
    "\n",
    "The main advantage of a CNN over a normal MLP (Multi-Layer Perceptron) is that instead of flattening a 2D image into a vector, we can actually feed as input the matrices to the CNNs.\n",
    "\n",
    "![cnn_v_mlp](img/cnn_v_mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what happens is that multiple portions of an image, are fed as input to hidden layers.  \n",
    "\n",
    "This is shown in this image of the digit 7.  \n",
    "The CNN actually visualises each of the 4 different colour-coded segments individually and trains itself accordingly.  \n",
    "\n",
    "![a](img/cnn_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Working\n",
    "\n",
    "We now have 2 collections of hidden nodes, according to the picture below.  \n",
    "Each of the 2 collections have nodes responsible for visualising and examining different portions of the image.  \n",
    "\n",
    "It is also good to have each of the hidden nodes within a collection to have a common group of weights.  \n",
    "The idea is that different regions within the image will share the same kind of information.  \n",
    "In other words, any pattern that's relevant towrads understanding the image, could appear anywhere within the image.  \n",
    "For instance, it is not mandatory for a photo of a cat, to have the face of the cat in the dead centre of the photo. The face could be anywhere, but then it would still be a photo of the cat, and our goal is to properly identify it.  \n",
    "\n",
    "![a](img/cnn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CNN can remember spatial information\n",
    "\n",
    "CNNs look at images as a whole or in patches and analyse groups of pixels at a time.  \n",
    "The key to preserving the spatial information is what is called the **convolutional layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer applies a series of image filters also called Convolutional Kernels to an input image.\n",
    "\n",
    "![4](img/cnn4.png)\n",
    "\n",
    "The resulting filtered images have different appearances. The filters may have extracted features like the edges of objects or the colours that distinguish the different classes of images. \n",
    "\n",
    "![5](img/cnn5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the case of digit recognition, this is how a CNN would perform its actions.\n",
    "\n",
    "![6](img/cnn6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters that define a Convoltuional Layer. \n",
    "\n",
    "##### This is important foundational knowledge.\n",
    "\n",
    "To detect changes in intensity in an image, you’ll be using and creating specific image filters that look at groups of pixels and react to alternating patterns of dark/light pixels. These filters produce an output that shows edges of objects and differing textures.  \n",
    "\n",
    "So, let’s take a closer look at these filters and see when they’re useful in processing images and identifying traits of interest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency in Images\n",
    "\n",
    "We have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s (Hz), and high pitches and made by high-frequency waves.  \n",
    "\n",
    "Examples of low and high-frequency sound waves are pictured below. On the y-axis is amplitude, which is a measure of sound pressure that corresponds to the perceived loudness of a sound and on the x-axis is time.  \n",
    "\n",
    "##### (Top image) a low frequency sound wave (bottom) a high frequency sound wave.\n",
    "\n",
    "![7](img/cnn7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High and Low Frequency\n",
    "\n",
    "Similarly, frequency in images is a **rate of change**. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next.   \n",
    "\n",
    "A low frequency image may be one that is relatively uniform in brightness or changes very slowly. This is easiest to see in an example.  \n",
    "\n",
    "##### High and Low Frequency Image Patterns\n",
    "\n",
    "![8](img/cnn8.png)\n",
    "\n",
    "Most images have both high-frequency and low-frequency components. In the image above, on the scarf and striped shirt, we have a high-frequency image pattern; this part changes very rapidly from one brightness to another. Higher up in this same image, we see parts of the sky and background that change very gradually, which is considered a smooth, low-frequency pattern.  \n",
    "\n",
    "**High-frequency components also correspond to the edges of objects in images**, which can help us classify those objects.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Handling\n",
    "\n",
    "Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge?   \n",
    "Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.  \n",
    "\n",
    "**Extend** The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.  \n",
    "\n",
    "**Padding** The image is padded with a border of 0's, black pixels.  \n",
    "\n",
    "**Crop** Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](img/filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Pass Filters\n",
    "\n",
    "> Used mainly for sharpening of the edges for clearer distinction.\n",
    "\n",
    "![a](img/filter1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the edge detection place take?\n",
    "\n",
    "![a](img/filter2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Kernels\n",
    "\n",
    "A matrix of numbers that modifies an image.\n",
    "\n",
    "![a](img/filter3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the convolutional kernel work?\n",
    "\n",
    "![a](img/filter4.png)\n",
    "![a](img/filter5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV\n",
    "\n",
    "Before we jump into coding our own convolutional kernels/filters, I'll introduce you to a new library that will be useful to use when dealing with computer vision tasks, such as image classification: OpenCV!  \n",
    "\n",
    "OpenCV is a computer vision and machine learning software library that includes many common image analysis algorithms that will help us build custom, intelligent computer vision applications. To start with, this includes tools that help us process images and select areas of interest! The library is widely used in academic and industrial applications; from their site, OpenCV includes an impressive list of users:  \n",
    "\n",
    "“Along with well-established companies like Google, Yahoo, Microsoft, Intel, IBM, Sony, Honda, Toyota that employ the library, there are many startups such as Applied Minds, VideoSurf, and Zeitera, that make extensive use of OpenCV.”  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Filters\n",
    "\n",
    "What you've just learned about different types of filters will be really important as you progress through this course, especially when you get to Convolutional Neural Networks (CNNs). CNNs are a kind of deep learning model that can learn to do things like image classification and object recognition. They keep track of spatial information and learn to extract features like the edges of objects in something called a convolutional layer. Below you'll see an simple CNN structure, made of multiple layers, below, including this \"convolutional layer\".\n",
    "\n",
    "![6](img/filter6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "The convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image.\n",
    "\n",
    "![7](img/filter7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, 4 different filters produce 4 differently filtered output images. When we stack these images, we form a complete convolutional layer with a depth of 4!\n",
    "\n",
    "![8](img/filter8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "In the code you've been working with, you've been setting the values of filter weights explicitly, but neural networks will actually learn the best filter weights as they train on a set of image data. You'll learn all about this type of neural network later in this section, but know that high-pass and low-pass filters are what define the behavior of a network like this, and you know how to code those from scratch!\n",
    "\n",
    "In practice, you'll also find that many neural networks learn to detect the edges of images because the edges of object contain valuable information about the shape of an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIlter Usage\n",
    "\n",
    "![9](img/filter9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking for multi-colour filters\n",
    "\n",
    "![10](img/filter10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride and Padding\n",
    "\n",
    "If the stride for each filter is such that at the end of a certain number of strides, there are blank nodes to traverse, just ignoring them would mean that our CNN does not have adequate data and information about certain portions of the image.  \n",
    "\n",
    "So Padding is very beneficial.\n",
    "\n",
    "![11](img/filter11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "### Before applying Maxpool layers\n",
    "\n",
    "![12](img/filter12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After MaxPooling\n",
    "\n",
    "![13](img/filter13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Kinds of Pooling\n",
    "\n",
    "some architectures choose to use average pooling, which chooses to average pixel values in a given window size. So in a 2x2 window, this operation will see 4 pixel values, and return a single, average of those four values, as output!\n",
    "\n",
    "This kind of pooling is typically not used for image classification problems because maxpooling is better at noticing the most important details about edges and other features in an image, but you may see this used in applications for which smoothing an image is preferable.\n",
    "\n",
    "### Average Pooling\n",
    "\n",
    "Chcek it out here: https://pytorch.org/docs/stable/nn.html#avgpool1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB vs Grayscale Depths\n",
    "\n",
    "![14](img/filter14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Depth to CNN data\n",
    "\n",
    "![15](img/filter15.png)\n",
    "![16](img/filter16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Padding is just adding a border of pixels around an image. In PyTorch, you specify the size of this border.\n",
    "\n",
    "#### Why do we need padding?\n",
    "\n",
    "When we create a convolutional layer, we move a square filter around an image, using a center-pixel as an anchor. So, this kernel cannot perfectly overlay the edges/corners of images. The nice feature of padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).\n",
    "\n",
    "The most common methods of padding are padding an image with all 0-pixels (zero padding) or padding them with the nearest pixel value. You can read more about calculating the amount of padding, given a kernel_size, here: http://cs231n.github.io/convolutional-networks/#conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Layer Documentations\n",
    "\n",
    "#### Convolutional Layers\n",
    "We typically define a convolutional layer in PyTorch using nn.Conv2d, with the following parameters, specified:\n",
    "\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)  \n",
    "in_channels refers to the depth of an input. For a grayscale image, this depth = 1  \n",
    "out_channels refers to the desired depth of the output, or the number of filtered images you want to get as output  \n",
    "kernel_size is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)  \n",
    "stride and padding have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y  \n",
    "Read more about Conv2d in the documentation:  https://pytorch.org/docs/stable/nn.html#conv2d\n",
    "\n",
    "#### Pooling Layers\n",
    "Maxpooling layers commonly come after convolutional layers to shrink the x-y dimensions of an input, read more about pooling layers in PyTorch, here: https://pytorch.org/docs/stable/nn.html#maxpool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Convolutional Layer in PyTorch\n",
    "\n",
    "![17](img/filter17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Depth but then the X and Y dimensions remain the same\n",
    "\n",
    "![18](img/filter18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to decrease the X and Y dimensions by half with increase in Depth, use Maxpool Layers again\n",
    "\n",
    "![19](img/filter19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer Creation\n",
    "\n",
    "To create a convolutional layer in PyTorch, you must first import the necessary module:\n",
    "\n",
    "import torch.nn as nn  \n",
    "Then, there is a two part process to defining a convolutional layer and defining the feedforward behavior of a model (how an input moves through the layers of a network. First you must define a Model class and fill in two functions.  \n",
    "\n",
    "You can define a convolutional layer in the __init__ function of by using the following format:  \n",
    "\n",
    "self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)  \n",
    "forward  \n",
    "\n",
    "Then, you refer to that layer in the forward function! Here, I am passing in an input image x and applying a ReLu function to the output of this layer.  \n",
    "\n",
    "x = F.relu(self.conv1(x))  \n",
    "Arguments  \n",
    "You must pass the following arguments:  \n",
    "\n",
    "in_channels - The number of inputs (in depth), 3 for an RGB image, for example.  \n",
    "out_channels - The number of output channels, i.e. the number of filtered \"images\" a convolutional layer is made of or the number of unique,  convolutional kernels that will be applied to an input.  \n",
    "kernel_size - Number specifying both the height and width of the (square) convolutional kernel.   \n",
    "There are some additional, optional arguments that you might like to tune:  \n",
    "\n",
    "stride - The stride of the convolution. If you don't specify anything, stride is set to 1.  \n",
    "padding - The border of 0's around an input array. If you don't specify anything, padding is set to 0.  \n",
    "NOTE: It is possible to represent both kernel_size and stride as either a number or a tuple.  \n",
    "\n",
    "There are many other tunable arguments that you can set to change the behavior of your convolutional layers. To read more about these, we recommend perusing the official documentation.   \n",
    "\n",
    "### Pooling Layers  \n",
    "\n",
    "Pooling layers take in a kernel_size and a stride. Typically the same value, the is the down-sampling factor. For example, the following code will down-sample an input's x-y dimensions, by a factor of 2:  \n",
    "\n",
    "self.pool = nn.MaxPool2d(2,2)  \n",
    "forward  \n",
    "\n",
    "Here, we see that poling layer being applied in the forward function.  \n",
    "\n",
    "x = F.relu(self.conv1(x))  \n",
    "x = self.pool(x)  \n",
    "\n",
    "### Convolutional Example 1\n",
    "\n",
    "Say I'm constructing a CNN, and my input layer accepts grayscale images that are 200 by 200 pixels (corresponding to a 3D array with height 200, width 200, and depth 1). Then, say I'd like the next layer to be a convolutional layer with 16 filters, each with a width and height of 2. When performing the convolution, I'd like the filter to jump two pixels at a time. I also don't want the filter to extend outside of the image boundaries; in other words, I don't want to pad the image with zeros. Then, to construct this convolutional layer, I would use the following line of code:  \n",
    "\n",
    "self.conv1 = nn.Conv2d(1, 16, 2, stride=2)  \n",
    "\n",
    "### Convolutional Example 2  \n",
    "\n",
    "Say I'd like the next layer in my CNN to be a convolutional layer that takes the layer constructed in Example 1 as input. Say I'd like my new layer to have 32 filters, each with a height and width of 3. When performing the convolution, I'd like the filter to jump 1 pixel at a time. I want this layer to have the same width and height as the input layer, ad so I will pad accordingly; the convolutional layer to see all regions of the previous layer, and I don't mind if the filter hangs over the edge of the previous layer when it's performing the convolution. Then, to construct this convolutional layer, I would use the following line of code:  \n",
    "\n",
    "self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  \n",
    "\n",
    "#### Convolution with 3x3 window and stride 1  \n",
    " \n",
    "![20](img/filter20.gif)\n",
    "\n",
    "### Sequential Models  \n",
    "\n",
    "We can also create a CNN in PyTorch by using a Sequential wrapper in the __init__ function. Sequential allows us to stack different types of layers, specifying activation functions in between!  \n",
    "\n",
    "def __init__(self):  \n",
    "        super(ModelName, self).__init__()  \n",
    "        self.features = nn.Sequential(  \n",
    "              nn.Conv2d(1, 16, 2, stride=2),  \n",
    "              nn.MaxPool2d(2, 2),  \n",
    "              nn.ReLU(True),  \n",
    "\n",
    "              nn.Conv2d(16, 32, 3, padding=1),  \n",
    "              nn.MaxPool2d(2, 2),  \n",
    "              nn.ReLU(True)   \n",
    "         )   \n",
    "Formula: Number of Parameters in a Convolutional Layer  \n",
    "The number of parameters in a convolutional layer depends on the supplied values of filters/out_channels, kernel_size, and input_shape. Let's define a few variables:  \n",
    "\n",
    "K - the number of filters in the convolutional layer  \n",
    "F - the height and width of the convolutional filters  \n",
    "D_in - the depth of the previous layer  \n",
    "Notice that K = out_channels, and F = kernel_size. Likewise, D_in is the last value in the input_shape tuple, typically 1 or 3 (RGB and grayscale, respectively).  \n",
    "\n",
    "Since there are F*F*D_in weights per filter, and the convolutional layer is composed of K filters, the total number of weights in the convolutional layer is K*F*F*D_in. Since there is one bias term per filter, the convolutional layer has K biases. Thus, the number of parameters in the convolutional layer is given by K*F*F*D_in + K.  \n",
    "\n",
    "Formula: Shape of a Convolutional Layer  \n",
    "The shape of a convolutional layer depends on the supplied values of kernel_size, input_shape, padding, and stride. Let's define a few variables:  \n",
    "\n",
    "K - the number of filters in the convolutional layer  \n",
    "F - the height and width of the convolutional filters  \n",
    "S - the stride of the convolution  \n",
    "P - the padding  \n",
    "W_in - the width/height (square) of the previous layer  \n",
    "Notice that K = out_channels, F = kernel_size, and S = stride. Likewise, W_in is the first and second value of the input_shape tuple.  \n",
    "\n",
    "The depth of the convolutional layer will always equal the number of filters K.  \n",
    "  \n",
    "The spatial dimensions of a convolutional layer can be calculates as: (W_in−F+2P)/S+1  \n",
    "\n",
    "### Flattening  \n",
    "\n",
    "Part of completing a CNN architecture, is to flatten the eventual output of a series of convolutional and pooling layers, so that all parameters can be seen (as a vector) by a linear classification layer. At this step, it is imperative that you know exactly how many parameters are output by a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
