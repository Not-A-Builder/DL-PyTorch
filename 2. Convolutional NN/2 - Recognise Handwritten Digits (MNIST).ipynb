{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can Deep Learning be used to identify a single object in an image.\n",
    "\n",
    "We want to design an image classifier that takes as input a hand-written digit and predicts what the digit actually is. The class ideally correctly identifies the given image.  \n",
    "\n",
    "To build this we will use the MNIST database:  \n",
    "A collection of 70,000 grayscale images of hand-written digits.  \n",
    "Each image depicts one of the digits 0 through to 9.  \n",
    "All the images are 28x28 pixels in dimensions.  \n",
    "This is perhaps one of the most famous databases in the fields of ML and DL.\n",
    "\n",
    "Using Deep Learning we will take a data driven approach to make use of algorithms to find patterns that will help us to distinguish one number from another.  \n",
    "\n",
    "Check out the most popular datasets of all time at https://www.kaggle.com/benhamner/popular-datasets-over-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MNIST database\n",
    "\n",
    "![MNIST](img/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Image Inputs\n",
    "\n",
    "Data normalization is an important pre-processing step. It ensures that each input (each pixel value, in this case) comes from a standard distribution. That is, the range of pixel values in one input image are the same as the range in another image. This standardization makes our model train and reach a minimum error, faster!  \n",
    "\n",
    "Data normalization is typically done by subtracting the mean (the average of all pixel values) from each pixel, and then dividing the result by the standard deviation of all the pixel values. Sometimes you'll see an approximation here, where we use a mean and standard deviation of 0.5 to center the pixel values. Read more about the Normalize transformation in PyTorch at https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-torch-tensor  \n",
    "\n",
    "The distribution of such data should resemble a Gaussian function (http://mathworld.wolfram.com/GaussianFunction.html) centered at zero. For image inputs we need the pixel numbers to be positive, so we often choose to scale the data in a normalized range [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Output Layers\n",
    "\n",
    "![io-layer](img/io-layer.png)\n",
    "\n",
    "#### Class Score Predictions\n",
    "\n",
    "![class-scores](img/class-scores.png)\n",
    "\n",
    "#### Layers' Architecture\n",
    "\n",
    "![CNN architecture](img/layer-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Solution foe hand-written-digit-classification\n",
    "\n",
    "The original code is to be found at https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "### The code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.2445 - acc: 0.9252 - val_loss: 0.0973 - val_acc: 0.9686\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.1029 - acc: 0.9688 - val_loss: 0.0812 - val_acc: 0.9751\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0749 - acc: 0.9768 - val_loss: 0.0758 - val_acc: 0.9790\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0599 - acc: 0.9824 - val_loss: 0.0728 - val_acc: 0.9801\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0494 - acc: 0.9846 - val_loss: 0.0801 - val_acc: 0.9794\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0435 - acc: 0.9870 - val_loss: 0.0775 - val_acc: 0.9808\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0376 - acc: 0.9888 - val_loss: 0.0765 - val_acc: 0.9814\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0351 - acc: 0.9897 - val_loss: 0.0778 - val_acc: 0.9832\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0319 - acc: 0.9908 - val_loss: 0.0714 - val_acc: 0.9853\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.0281 - acc: 0.9919 - val_loss: 0.0899 - val_acc: 0.9839\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.0276 - acc: 0.9924 - val_loss: 0.0868 - val_acc: 0.9823\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0258 - acc: 0.9927 - val_loss: 0.1095 - val_acc: 0.9803\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0230 - acc: 0.9939 - val_loss: 0.0950 - val_acc: 0.9816\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0224 - acc: 0.9937 - val_loss: 0.0990 - val_acc: 0.9829\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0212 - acc: 0.9943 - val_loss: 0.1141 - val_acc: 0.9808\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.0209 - acc: 0.9942 - val_loss: 0.1083 - val_acc: 0.9840\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0194 - acc: 0.9948 - val_loss: 0.1277 - val_acc: 0.9808\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.0184 - acc: 0.9950 - val_loss: 0.1115 - val_acc: 0.9829\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.0174 - acc: 0.9951 - val_loss: 0.1116 - val_acc: 0.9842\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.0164 - acc: 0.9957 - val_loss: 0.1322 - val_acc: 0.9811\n",
      "Test loss: 0.13219076714993774\n",
      "Test accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist     # Import MNIST dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)     # Flatten each input image into a vector of size 784\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))     # Hidden Layer 1\n",
    "model.add(Dropout(0.2))     # Dropout Layer 1 - 20% chance of dropout (nodes being dropped randomly)\n",
    "model.add(Dense(512, activation='relu'))     # Hidden Layer 2\n",
    "model.add(Dropout(0.2))     # Dropout Layer 2 - these layers help avoid overfitting\n",
    "model.add(Dense(num_classes, activation='softmax'))     # num_classes is the output layer of 10 nodes\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![do-research](img/do-research.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is just one solution to the task of hand-written digit classification.\n",
    "\n",
    "The next step in a problem like this, is to:  \n",
    "1. Keep looking for better solutions or structures, or  \n",
    "2. If we do find a model that better appeals to us, it's best to try them out in code and see how well they perform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
