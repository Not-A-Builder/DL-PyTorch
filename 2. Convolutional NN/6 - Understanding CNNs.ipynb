{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of CNNs?\n",
    "\n",
    "The main advantage of a CNN over a normal MLP (Multi-Layer Perceptron) is that instead of flattening a 2D image into a vector, we can actually feed as input the matrices to the CNNs.\n",
    "\n",
    "![cnn_v_mlp](img/cnn_v_mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what happens is that multiple portions of an image, are fed as input to hidden layers.  \n",
    "\n",
    "This is shown in this image of the digit 7.  \n",
    "The CNN actually visualises each of the 4 different colour-coded segments individually and trains itself accordingly.  \n",
    "\n",
    "![a](img/cnn_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Working\n",
    "\n",
    "We now have 2 collections of hidden nodes, according to the picture below.  \n",
    "Each of the 2 collections have nodes responsible for visualising and examining different portions of the image.  \n",
    "\n",
    "It is also good to have each of the hidden nodes within a collection to have a common group of weights.  \n",
    "The idea is that different regions within the image will share the same kind of information.  \n",
    "In other words, any pattern that's relevant towrads understanding the image, could appear anywhere within the image.  \n",
    "For instance, it is not mandatory for a photo of a cat, to have the face of the cat in the dead centre of the photo. The face could be anywhere, but then it would still be a photo of the cat, and our goal is to properly identify it.  \n",
    "\n",
    "![a](img/cnn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CNN can remember spatial information\n",
    "\n",
    "CNNs look at images as a whole or in patches and analyse groups of pixels at a time.  \n",
    "The key to preserving the spatial information is what is called the **convolutional layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer applies a series of image filters also called Convolutional Kernels to an input image.\n",
    "\n",
    "![4](img/cnn4.png)\n",
    "\n",
    "The resulting filtered images have different appearances. The filters may have extracted features like the edges of objects or the colours that distinguish the different classes of images. \n",
    "\n",
    "![5](img/cnn5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the case of digit recognition, this is how a CNN would perform its actions.\n",
    "\n",
    "![6](img/cnn6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters that define a Convoltuional Layer. \n",
    "\n",
    "##### This is important foundational knowledge.\n",
    "\n",
    "To detect changes in intensity in an image, you’ll be using and creating specific image filters that look at groups of pixels and react to alternating patterns of dark/light pixels. These filters produce an output that shows edges of objects and differing textures.  \n",
    "\n",
    "So, let’s take a closer look at these filters and see when they’re useful in processing images and identifying traits of interest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency in Images\n",
    "\n",
    "We have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s (Hz), and high pitches and made by high-frequency waves.  \n",
    "\n",
    "Examples of low and high-frequency sound waves are pictured below. On the y-axis is amplitude, which is a measure of sound pressure that corresponds to the perceived loudness of a sound and on the x-axis is time.  \n",
    "\n",
    "##### (Top image) a low frequency sound wave (bottom) a high frequency sound wave.\n",
    "\n",
    "![7](img/cnn7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High and Low Frequency\n",
    "\n",
    "Similarly, frequency in images is a **rate of change**. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next.   \n",
    "\n",
    "A low frequency image may be one that is relatively uniform in brightness or changes very slowly. This is easiest to see in an example.  \n",
    "\n",
    "##### High and Low Frequency Image Patterns\n",
    "\n",
    "![8](img/cnn8.png)\n",
    "\n",
    "Most images have both high-frequency and low-frequency components. In the image above, on the scarf and striped shirt, we have a high-frequency image pattern; this part changes very rapidly from one brightness to another. Higher up in this same image, we see parts of the sky and background that change very gradually, which is considered a smooth, low-frequency pattern.  \n",
    "\n",
    "**High-frequency components also correspond to the edges of objects in images**, which can help us classify those objects.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Handling\n",
    "\n",
    "Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge?   \n",
    "Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.  \n",
    "\n",
    "**Extend** The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.  \n",
    "\n",
    "**Padding** The image is padded with a border of 0's, black pixels.  \n",
    "\n",
    "**Crop** Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](img/filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Pass Filters\n",
    "\n",
    "> Used mainly for sharpening of the edges for clearer distinction.\n",
    "\n",
    "![a](img/filter1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the edge detection place take?\n",
    "\n",
    "![a](img/filter2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Kernels\n",
    "\n",
    "A matrix of numbers that modifies an image.\n",
    "\n",
    "![a](img/filter3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the convolutional kernel work?\n",
    "\n",
    "![a](img/filter4.png)\n",
    "![a](img/filter5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV\n",
    "\n",
    "Before we jump into coding our own convolutional kernels/filters, I'll introduce you to a new library that will be useful to use when dealing with computer vision tasks, such as image classification: OpenCV!  \n",
    "\n",
    "OpenCV is a computer vision and machine learning software library that includes many common image analysis algorithms that will help us build custom, intelligent computer vision applications. To start with, this includes tools that help us process images and select areas of interest! The library is widely used in academic and industrial applications; from their site, OpenCV includes an impressive list of users:  \n",
    "\n",
    "“Along with well-established companies like Google, Yahoo, Microsoft, Intel, IBM, Sony, Honda, Toyota that employ the library, there are many startups such as Applied Minds, VideoSurf, and Zeitera, that make extensive use of OpenCV.”  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Filters\n",
    "\n",
    "What you've just learned about different types of filters will be really important as you progress through this course, especially when you get to Convolutional Neural Networks (CNNs). CNNs are a kind of deep learning model that can learn to do things like image classification and object recognition. They keep track of spatial information and learn to extract features like the edges of objects in something called a convolutional layer. Below you'll see an simple CNN structure, made of multiple layers, below, including this \"convolutional layer\".\n",
    "\n",
    "![6](img/filter6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "The convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image.\n",
    "\n",
    "![7](img/filter7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, 4 different filters produce 4 differently filtered output images. When we stack these images, we form a complete convolutional layer with a depth of 4!\n",
    "\n",
    "![8](img/filter8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "In the code you've been working with, you've been setting the values of filter weights explicitly, but neural networks will actually learn the best filter weights as they train on a set of image data. You'll learn all about this type of neural network later in this section, but know that high-pass and low-pass filters are what define the behavior of a network like this, and you know how to code those from scratch!\n",
    "\n",
    "In practice, you'll also find that many neural networks learn to detect the edges of images because the edges of object contain valuable information about the shape of an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIlter Usage\n",
    "\n",
    "![9](img/filter9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
